{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Text Generaion\n",
    "\n",
    "This program runs on a corpus of homogeneous document of textual content and automatically generates similar text from a given seed. Specifically, we apply on a set of 5000 user Amazon reviews on a product. There are three  sequential modeling techniques to be chosen from - RNN, LSTM< and GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# to print all in a large list avoiding \"...\"\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling parameters\n",
    "\n",
    "EMBEDDING_DIMENSION = 300\n",
    "RNN_OUTPUT_DIMENSION = 50\n",
    "MEMORY_LENGTH = 25\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 1024\n",
    "TRAIN_PERCENT = 0.7\n",
    "VERBOSE_MODE = True\n",
    "SAVE_MODEL = True\n",
    "SAVED_MODEL = './models/review_model_lstm.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  ID  \\\n",
      "0  Review: 4736 : Amazon.com: Customer reviews: F...   \n",
      "1  Review: 2571 : Amazon.com: Customer reviews: F...   \n",
      "2  Review: 1356 : Amazon.com: Customer reviews: F...   \n",
      "3  Review: 573 : Amazon.com: Customer reviews: Fi...   \n",
      "4  Review: 43 : Amazon.com: Customer reviews: Fir...   \n",
      "\n",
      "                           Date  \\\n",
      "0  Tue Apr 16 00:00:00 EDT 2019   \n",
      "1  Sat Oct 27 00:00:00 EDT 2018   \n",
      "2  Tue Nov 13 00:00:00 EST 2018   \n",
      "3  Fri Nov 09 00:00:00 EST 2018   \n",
      "4  Sat Oct 06 00:00:00 EDT 2018   \n",
      "\n",
      "                                                Body  \n",
      "0  I like that it has a feature that lets you set...  \n",
      "1  I returned this tablet for the simple fact tha...  \n",
      "2  I guess you get whatcha pay for. I thought, wi...  \n",
      "3  I had the Kindle Fire HDX 8.9. It had so many ...  \n",
      "4  Sound is tiny and extremely difficult to keep ...  \n",
      "Number of Reviews:  5000\n",
      "I had the Kindle Fire HDX 8.9. It had so many more apps. One which I truly miss is, Optimum to go. I am and older person and I liked that I could watch my TV in my bedroom(no reg Tv in my bedroom). I played scrabble , this kindle does not have it either. I am truly disappointed in this. I might give it to my grandson and get myself another brand.. Alexa is not that important to me.\n",
      "I had the Kindle Fire HDX 8.9. It had so many more apps. One which I truly miss is, Optimum to go. I am and older person and I liked that I could watch my TV in my bedroom(no reg Tv in my bedroom). I played scrabble , this kindle does not have it either. I am truly disappointed in this. I might give it to my grandson and get myself another brand.. Alexa is not that important to me.\n"
     ]
    }
   ],
   "source": [
    "# Read the review file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data = pd.read_csv('./data/Amazon Reviews - Fire HD 8 Tablet - 200.csv') # for development\n",
    "data = pd.read_csv(r'C:\\Users\\aksha\\Downloads\\Amazon Reviews - Fire HD 8 Tablet - 5000.csv')\n",
    "print(data.head())\n",
    "\n",
    "# Extract abstracts\n",
    "all_reviews = list(data['Body'])\n",
    "print(\"Number of Reviews: \", len(all_reviews))\n",
    "\n",
    "# test print and cross check\n",
    "print(data['Body'][3])\n",
    "print(all_reviews[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\aksha\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  ID  \\\n",
      "0  Review: 4736 : Amazon.com: Customer reviews: F...   \n",
      "1  Review: 2571 : Amazon.com: Customer reviews: F...   \n",
      "2  Review: 1356 : Amazon.com: Customer reviews: F...   \n",
      "3  Review: 573 : Amazon.com: Customer reviews: Fi...   \n",
      "4  Review: 43 : Amazon.com: Customer reviews: Fir...   \n",
      "\n",
      "                           Date  \\\n",
      "0  Tue Apr 16 00:00:00 EDT 2019   \n",
      "1  Sat Oct 27 00:00:00 EDT 2018   \n",
      "2  Tue Nov 13 00:00:00 EST 2018   \n",
      "3  Fri Nov 09 00:00:00 EST 2018   \n",
      "4  Sat Oct 06 00:00:00 EDT 2018   \n",
      "\n",
      "                                                Body        yymm  \n",
      "0  I like that it has a feature that lets you set...   (2019, 4)  \n",
      "1  I returned this tablet for the simple fact tha...  (2018, 10)  \n",
      "2  I guess you get whatcha pay for. I thought, wi...  (2018, 11)  \n",
      "3  I had the Kindle Fire HDX 8.9. It had so many ...  (2018, 11)  \n",
      "4  Sound is tiny and extremely difficult to keep ...  (2018, 10)  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the frequency of reviews on a monthly basis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data['yymm'] = [\n",
    "    (year, month) for year, month in zip(\n",
    "        pd.DatetimeIndex(data['Date']).year, pd.DatetimeIndex(data['Date']).month)\n",
    "]\n",
    "\n",
    "monthly = data.groupby('yymm')['ID'].count().plot.bar(\n",
    "    color='red', edgecolor='k', figsize=(12, 6))\n",
    "\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Reviews over Time')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aText'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5520/3941657600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# \"pip install atext\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0maText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# kill the spawned JVM process once done with aText, else it will continue to running in the background\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'aText'"
     ]
    }
   ],
   "source": [
    "#exclude\n",
    "# Load the text analysis package aText\n",
    "\n",
    "# To install aText: \"conda env list\" to list all conda environment\n",
    "# \"activate py37\" to activate your favorite environment\n",
    "# \"pip install atext\"\n",
    "\n",
    "import aText\n",
    "\n",
    "# kill the spawned JVM process once done with aText, else it will continue to running in the background\n",
    "aText.stop_atext()\n",
    "\n",
    "# first usage of aText takes time to download some necessary files\n",
    "# make sure to have write permission for the Lib/site-package directory\n",
    "aText.start_atext()\n",
    "aText.test()\n",
    "\n",
    "from py4j.java_gateway import JavaGateway\n",
    "gateway = JavaGateway()                 # connect to the JV\n",
    "misc_app = gateway.entry_point          # get the an application instance\n",
    "\n",
    "data['sentiment'] = [misc_app.sentiment(text)[0] for text in data['Body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude\n",
    "# Plot the overall sentiment in reviews on a monthly basis\n",
    "\n",
    "monthly = data.groupby('yymm') \\\n",
    "    .agg({'sentiment':'mean', 'ID':'size'})\\\n",
    "    .rename(columns={'sentiment':'avg_sent','ID':'count'}) \n",
    "    #.reset_index()\n",
    "print(monthly)\n",
    "\n",
    "#monthly['sentiment'].plot.bar(color='red', edgecolor='k', figsize=(12, 6))\n",
    "\n",
    "freq_series = pd.Series(monthly['avg_sent'])\n",
    "\n",
    "plt.figure(edgecolor='k', figsize=(12, 8))\n",
    "ax = freq_series.plot(kind = 'bar')\n",
    "ax.set_ylabel('Monthly Average Sentiment Level')\n",
    "ax.set_xlabel('Year & Month')\n",
    "ax.set_title('Monthly Sentiment Chart')\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "for rect, label in zip(rects, monthly['count']):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height, label,\n",
    "            ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For word embedding, either train with the corpus of reviews or load a pretrained model from Google\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load Google's pre-trained Word2Vec, this takes a minute or two\n",
    "model_path = r'C:\\Users\\aksha\\Downloads\\GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True) \n",
    "    \n",
    "# Training\n",
    "# size: The number of dimensions of the embeddings and the default is 100.\n",
    "# window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "# min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "# workers: The number of partitions during training and the default workers is 3.\n",
    "# sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "\n",
    "#word2vec_model = Word2Vec(review_sentences, size=100, window=50, min_count=1, workers=4, sg=1)\n",
    "#word2vec_model.save(\"./models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subrata', 'loves', 'data', 'science', '.', 'He', 'lives', 'in', 'Belmont', '.']\n",
      "['Subrata loves data science.', 'He lives in Belmont.']\n",
      "['Ways toooooo many ads, screen is ok.', 'I am actually sorry I purchased this unit.']\n",
      "[['Ways', 'toooooo', 'many', 'ads', ',', 'screen', 'is', 'ok', '.'], ['I', 'am', 'actually', 'sorry', 'I', 'purchased', 'this', 'unit', '.']]\n",
      "[['This', 'is', 'our', '9th', 'Kindle', '.', 'I', 'broke', 'the', 'glass', 'on', 'my', 'last', 'Fire', 'HD8', 'and', 'needed', 'another', '.', 'I', 'had', 'a', 'minor', 'issue', 'with', 'this', 'new', 'one', 'and', 'Amazon', 'called', 'ME', ',', 'problem', 'was', 'easily', 'solved', '.'], ['I', 'bought', 'this', 'to', 'replace', 'a', 'three', 'year', 'old', 'Tablet', 'that', 'I', 'paid', '$', '120', 'for', '.', 'I', 'was', 'hoping', 'that', 'this', 'tablet', 'would', 'be', 'faster', 'and', 'my', 'old', 'tablet', 'was', 'done', 'updating', '.', 'For', '$', '50', 'this', 'Amazon', 'Tablet', 'has', 'blown', 'away', 'all', 'my', 'previous', 'tablets', 'that', 'costed', 'double', 'or', 'more', '.']]\n",
      "['very', 'nice', 'tablet', 'good', 'size', 'fits', 'in', 'my', 'purse', 'good', 'amount', 'memory', 'has', 'Alexa', 'feature', 'only', 'thing', 'I', 'would', 'change', 'is', 'the', 'volume', 'the', 'speaker', 'is', 'not', 'loud', 'enough', 'for', 'me', 'when', 'I', 'watch', 'videos', 'or', 'Sling', 'TV', 'Maybe', 'it', 'personal', 'choice']\n",
      "['THAN', 'THANK', 'THANKS', 'THAT', 'THE', 'THEIR', 'THEM', 'THEN', 'THERE', 'THESE', 'THEY', 'THIN', 'THING', 'THINGS', 'THIS', 'THOUGH', 'THREE', 'THis', 'TIME', 'TIMES', 'TINY', 'TL', 'TO', 'TON', 'TONS', 'TOO', 'TOP', 'TOPS', 'TOTAL', 'TOUCH', 'TOY', 'TRAVELING', 'TRUE', 'TRUST', 'TS', 'TSA', 'TURN', 'TV', 'TVs', 'TWO', 'TYPE', 'TYPING', 'Tab', 'Table', 'Tables', 'Tablet', 'Tablets', 'Tabs', 'Tailoring']\n",
      "5188\n"
     ]
    }
   ],
   "source": [
    "# Basic parsing using NLTK and the preparation of data\n",
    "# Can also be used \"from keras.preprocessing.text import Tokenizer\" for parsing\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data = \"Subrata loves data science. He lives in Belmont.\"\n",
    "print(word_tokenize(data))\n",
    "print(sent_tokenize(data))\n",
    "\n",
    "# extract sentences but not really used\n",
    "review_sentences = []\n",
    "for sentence in [sent_tokenize(r) for r in all_reviews]:\n",
    "    review_sentences += sentence\n",
    "print(review_sentences[18:20])\n",
    "review_sentences = [word_tokenize(s) for s in review_sentences]\n",
    "print(review_sentences[18:20])\n",
    "\n",
    "# Tokenize\n",
    "reviews_tokenized = [word_tokenize(s) for s in all_reviews]\n",
    "print(reviews_tokenized[18:20])\n",
    "\n",
    "# Filter out the ones not in the mode but keeping the common punctuations: '.!,-:;'\n",
    "reviews_tokenized = [[w for w in s if w in word2vec_model.key_to_index] for s in reviews_tokenized]\n",
    "print(reviews_tokenized[103])\n",
    "\n",
    "# Build the set of all words of the words and order\n",
    "review_words_set = set([])\n",
    "for s in reviews_tokenized:\n",
    "    review_words_set.update(s)\n",
    "review_words_sorted = sorted(review_words_set)\n",
    "print(review_words_sorted[3001:3050])\n",
    "\n",
    "# build an index of the words\n",
    "review_words_indexed = {w:i for i, w in enumerate(review_words_sorted)}\n",
    "print(review_words_indexed['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12603, 300)\n"
     ]
    }
   ],
   "source": [
    "# construct the embedding matrix to input to the model\n",
    "embedding_matrix = np.zeros((len(review_words_sorted), EMBEDDING_DIMENSION))\n",
    "for i, word in enumerate(review_words_sorted):\n",
    "    embedding_matrix[i] = word2vec_model[word]\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208514, 25) (89364, 25) (208514,) (89364,)\n",
      "['Unfortunately' 'it' 'may' 'be' 'returned' 'Feels' 'way' 'flimsy' 'poor'\n",
      " 'quality' 'I' 'wish' 'this' 'would' 'been' 'nicer' 'really' 'wanted'\n",
      " 'become' 'more' 'loyal' 'Amazon' 'ecosystem' 'this' 'item']\n",
      "wo\n",
      "['purchased' 'this' 'new' 'kindle' 'specifically' 'because' 'it' 'claims'\n",
      " 'support' 'up' 'GB' 'compatible' 'micro' 'sd' 'card' 'The' 'kindle' 'I'\n",
      " 'received' 'would' 'not' 'communicate' 'with' 'the' 'card']\n",
      "for\n"
     ]
    }
   ],
   "source": [
    "# generate training and validation sequences of words from each review\n",
    "\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "# Iterate through the sequences of tokens\n",
    "for seq in reviews_tokenized:\n",
    "    # Create multiple training examples from each sequence\n",
    "    for i in range(MEMORY_LENGTH, len(seq)):\n",
    "        # Extract the sub sequence and label\n",
    "        extract = seq[i - MEMORY_LENGTH:i + 1]\n",
    "\n",
    "        # Set the features and label\n",
    "        #all_sequences.append(' '.join(extract[:-1]))\n",
    "        all_sequences.append(extract[:-1])\n",
    "        all_labels.append(extract[-1])\n",
    "        \n",
    "# Decide on number of samples for training\n",
    "train_end = int(TRAIN_PERCENT * len(all_labels))\n",
    "\n",
    "train_sequences_words = np.array(all_sequences[:train_end])\n",
    "valid_sequences_words = np.array(all_sequences[train_end:])\n",
    "\n",
    "train_labels_words = np.array(all_labels[:train_end])\n",
    "valid_labels_words = np.array(all_labels[train_end:])\n",
    "\n",
    "print(train_sequences_words.shape, valid_sequences_words.shape,\n",
    "      train_labels_words.shape, valid_labels_words.shape)\n",
    "print(train_sequences_words[190])\n",
    "print(train_labels_words[190])\n",
    "print(valid_sequences_words[200])\n",
    "print(valid_labels_words[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208514, 25) (208514, 12603) (89364, 25) (89364, 12603)\n",
      "[ 8648  5484  6709   658 11574  6616  8916 12331  6989 11095 12080  8913\n",
      "   658  5487 11583 12367  4356 12520  8796  4792  8967 11876  8913 10886\n",
      " 12331]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[ 3409  8648  8904  8916  7171 11571 11615  5886  8655  7789  4985 12381\n",
      "  7304  2045  8904  8916  6038  6619  7508  8648  7125 11615  8916  7789\n",
      "  4349]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# generate training and validation sequences of indexes from each review\n",
    "\n",
    "train_sequences = np.array([[review_words_indexed[w] for w in s] for s in train_sequences_words])\n",
    "train_labels = np.array([np.zeros(len(review_words_indexed))] * len(train_labels_words))\n",
    "for i, w in enumerate(train_labels_words):\n",
    "    train_labels[i, review_words_indexed[w]] = 1\n",
    "\n",
    "valid_sequences = np.array([[review_words_indexed[w] for w in s] for s in valid_sequences_words])\n",
    "valid_labels = np.array([np.zeros(len(review_words_indexed))] * len(valid_labels_words))\n",
    "for i, w in enumerate(valid_labels_words):\n",
    "    valid_labels[i, review_words_indexed[w]] = 1\n",
    "\n",
    "print(train_sequences.shape, train_labels.shape, valid_sequences.shape, valid_labels.shape)\n",
    "print(train_sequences[19000])\n",
    "print(train_labels[19000])\n",
    "print(valid_sequences[5000])\n",
    "print(valid_labels[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 300)         3780900   \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 300)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                70200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6528      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12603)             1625787   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,483,415\n",
      "Trainable params: 1,702,515\n",
      "Non-trainable params: 3,780,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the RNN models\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Masking, Dropout, Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# Map words to an embedding via word2vec\n",
    "lstm_model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(review_words_sorted),\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        mask_zero=True))\n",
    "lstm_model.add(Masking())\n",
    "\n",
    "lstm_model.add(\n",
    "    LSTM(\n",
    "        RNN_OUTPUT_DIMENSION,\n",
    "        return_sequences=False,\n",
    "        dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "lstm_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "lstm_model.add(Dense(len(review_words_sorted), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def make_callbacks(model_name):\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=25)]\n",
    "    if SAVE_MODEL:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                model_name,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208514, 25) (89364, 25) (208514, 12603) (89364, 12603)\n",
      "Epoch 1/25\n",
      "204/204 [==============================] - 120s 574ms/step - loss: 6.9646 - accuracy: 0.0390 - val_loss: 6.6429 - val_accuracy: 0.0447\n",
      "Epoch 2/25\n",
      "204/204 [==============================] - 95s 464ms/step - loss: 6.5805 - accuracy: 0.0491 - val_loss: 6.5049 - val_accuracy: 0.0553\n",
      "Epoch 3/25\n",
      "204/204 [==============================] - 98s 479ms/step - loss: 6.3996 - accuracy: 0.0643 - val_loss: 6.3069 - val_accuracy: 0.0736\n",
      "Epoch 4/25\n",
      "204/204 [==============================] - 98s 482ms/step - loss: 6.2147 - accuracy: 0.0762 - val_loss: 6.1613 - val_accuracy: 0.0811\n",
      "Epoch 5/25\n",
      "204/204 [==============================] - 100s 491ms/step - loss: 6.0828 - accuracy: 0.0814 - val_loss: 6.0605 - val_accuracy: 0.0868\n",
      "Epoch 6/25\n",
      "204/204 [==============================] - 99s 484ms/step - loss: 5.9805 - accuracy: 0.0852 - val_loss: 5.9877 - val_accuracy: 0.0918\n",
      "Epoch 7/25\n",
      "204/204 [==============================] - 102s 501ms/step - loss: 5.8991 - accuracy: 0.0900 - val_loss: 5.9340 - val_accuracy: 0.0992\n",
      "Epoch 8/25\n",
      "204/204 [==============================] - 105s 516ms/step - loss: 5.8321 - accuracy: 0.0945 - val_loss: 5.8893 - val_accuracy: 0.1020\n",
      "Epoch 9/25\n",
      "204/204 [==============================] - 100s 492ms/step - loss: 5.7708 - accuracy: 0.0988 - val_loss: 5.8574 - val_accuracy: 0.1067\n",
      "Epoch 10/25\n",
      "204/204 [==============================] - 100s 493ms/step - loss: 5.7172 - accuracy: 0.1015 - val_loss: 5.8168 - val_accuracy: 0.1089\n",
      "Epoch 11/25\n",
      "204/204 [==============================] - 100s 489ms/step - loss: 5.6660 - accuracy: 0.1046 - val_loss: 5.7848 - val_accuracy: 0.1133\n",
      "Epoch 12/25\n",
      "204/204 [==============================] - 97s 478ms/step - loss: 5.6172 - accuracy: 0.1082 - val_loss: 5.7628 - val_accuracy: 0.1150\n",
      "Epoch 13/25\n",
      "204/204 [==============================] - 97s 477ms/step - loss: 5.5739 - accuracy: 0.1100 - val_loss: 5.7372 - val_accuracy: 0.1181\n",
      "Epoch 14/25\n",
      "204/204 [==============================] - 100s 489ms/step - loss: 5.5335 - accuracy: 0.1116 - val_loss: 5.7203 - val_accuracy: 0.1202\n",
      "Epoch 15/25\n",
      "204/204 [==============================] - 95s 468ms/step - loss: 5.4949 - accuracy: 0.1137 - val_loss: 5.7050 - val_accuracy: 0.1217\n",
      "Epoch 16/25\n",
      "204/204 [==============================] - 97s 478ms/step - loss: 5.4627 - accuracy: 0.1160 - val_loss: 5.6866 - val_accuracy: 0.1224\n",
      "Epoch 17/25\n",
      "204/204 [==============================] - 97s 475ms/step - loss: 5.4275 - accuracy: 0.1173 - val_loss: 5.6752 - val_accuracy: 0.1237\n",
      "Epoch 18/25\n",
      "204/204 [==============================] - 100s 488ms/step - loss: 5.3968 - accuracy: 0.1187 - val_loss: 5.6663 - val_accuracy: 0.1260\n",
      "Epoch 19/25\n",
      "204/204 [==============================] - 99s 487ms/step - loss: 5.3674 - accuracy: 0.1202 - val_loss: 5.6576 - val_accuracy: 0.1263\n",
      "Epoch 20/25\n",
      "204/204 [==============================] - 99s 486ms/step - loss: 5.3397 - accuracy: 0.1209 - val_loss: 5.6449 - val_accuracy: 0.1274\n",
      "Epoch 21/25\n",
      "204/204 [==============================] - 97s 475ms/step - loss: 5.3155 - accuracy: 0.1218 - val_loss: 5.6465 - val_accuracy: 0.1284\n",
      "Epoch 22/25\n",
      "204/204 [==============================] - 99s 487ms/step - loss: 5.2889 - accuracy: 0.1227 - val_loss: 5.6501 - val_accuracy: 0.1303\n",
      "Epoch 23/25\n",
      "204/204 [==============================] - 100s 489ms/step - loss: 5.2661 - accuracy: 0.1234 - val_loss: 5.6464 - val_accuracy: 0.1305\n",
      "Epoch 24/25\n",
      "204/204 [==============================] - 102s 500ms/step - loss: 5.2425 - accuracy: 0.1246 - val_loss: 5.6447 - val_accuracy: 0.1310\n",
      "Epoch 25/25\n",
      "204/204 [==============================] - 104s 511ms/step - loss: 5.2181 - accuracy: 0.1244 - val_loss: 5.6387 - val_accuracy: 0.1322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28af4158040>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "print(train_sequences.shape, valid_sequences.shape, train_labels.shape, valid_labels.shape)\n",
    "lstm_model.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE_MODE,\n",
    "    callbacks=make_callbacks(SAVED_MODEL),\n",
    "    validation_data=(valid_sequences, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very nice tablet good size fits in my purse good amount memory has Alexa feature only thing I would change is the volume the speaker is not loud enough for me when I watch videos or Sling TV Maybe it personal choice\n",
      "I am truly disappointed in this tablet I have had the Fire HD 8 I have had the Fire HD 8 I have had the Fire HD 8 I have had\n"
     ]
    }
   ],
   "source": [
    "# generate reviews from a trained model\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_review(model,\n",
    "                    seed_words,\n",
    "                    training_length=MEMORY_LENGTH,\n",
    "                    new_words=25):\n",
    "\n",
    "    # index the given sequence\n",
    "    seed = [review_words_indexed[w] for w in seed_words.split(' ')]\n",
    "\n",
    "    generated = seed\n",
    "    # Keep adding new words\n",
    "    for i in range(new_words):\n",
    "\n",
    "        # Make a prediction from the seed\n",
    "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "        next_index = np.argmax(preds)\n",
    "\n",
    "        # New seed adds on old word\n",
    "        seed = seed[1:] + [next_index]\n",
    "        generated.append(next_index)\n",
    "\n",
    "    # returning generated and actual reviews\n",
    "    generated = [review_words_sorted[i] for i in generated]\n",
    "    return ' '.join(generated)\n",
    "\n",
    "original_sequence = reviews_tokenized[103]\n",
    "print(' '.join(original_sequence))\n",
    "#generated = generate_review(lstm_model,  'very nice tablet good size fits in my bag')\n",
    "generated = generate_review(lstm_model,  'I am truly disappointed in this')\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
